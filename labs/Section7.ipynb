{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# special IPython command to prepare the notebook for matplotlib\n",
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "\n",
    "# special matplotlib command for global plot configuration\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['figure.dpi'] = 150\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['font.size'] = 14\n",
    "rcParams['patch.edgecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Classification with Decision Trees, Random Forests, and SVM\n",
    "\n",
    "### Last time\n",
    "- PCA\n",
    "\n",
    "### Today\n",
    "- Decision Trees (with bagging)\n",
    "- Random Forests\n",
    "- Cross validation\n",
    "\n",
    "## Note - this notebook works best in python 2.7 because pydot is currently only supported for python 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into training and testing\n",
    "\n",
    "For this problem we'll use the included iris dataset and use cross validation to help select the parameters for our decision tree classifier.  The [decision tree classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) is part of the sklearn library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the iris data set\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data  \n",
    "Y = iris.target\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a train and a test set. Use a random selection of 33% of the samples as test data. Sklearn provides the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) function for this purpose. Print the dimensions of all the train and test data sets you have created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put test data aside\n",
    "from sklearn import cross_validation\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(\n",
    "    X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA to help visualize the groups...\n",
    "\n",
    "We can examine the data a little further by looking at the projections to the first two principal components of the data.  To do this we'll use Principal Component Analysis [(PCA)](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and create a scatter plot of the result.  We can then color the data according to the type of iris. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a scatter plot of the data in two dimensions\n",
    "svd = decomposition.PCA(n_components=2)\n",
    "X_train_centered = X_train - np.mean(X_train, axis=0)\n",
    "X_2d = svd.fit_transform(X_train_centered)\n",
    "\n",
    "plt.scatter(X_2d[:,0], X_2d[:,1], c=Y_train, s = 50, cmap=plt.cm.prism)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('First two PCs using iris data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "What is the best depth for a decision tree on the four features provided by the iris dataset?\n",
    "\n",
    "We'll use the [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) from sklearn to model the iris dataset.  Decision tress are widely used classifiers because:\n",
    "- they're simple to understand an interpret\n",
    "- require little data prep\n",
    "- processing cost is on the lower end of most classifiers (depends on size of dataset)\n",
    "- handles both numerical and categorical data\n",
    "- handles multi-output problems\n",
    "- validation can be accomplished with [statistical tests](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.score)\n",
    "\n",
    "Drawbacks of Decision Trees though include:\n",
    "- can overfit the data and create overly complex trees\n",
    "- unstable from noisy (or complex) data\n",
    "- unbalanced dataset classes will create biased trees (we'll see this later...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = []\n",
    "\n",
    "for n in range(1,5):\n",
    "    # create a model with training data \n",
    "    clf = DecisionTreeClassifier(max_depth=n).fit(X_train, Y_train)\n",
    "    # now score it on the test data set\n",
    "    scores.append(clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "fig.set_size_inches(12,6)\n",
    "\n",
    "plt.plot(scores)\n",
    "\n",
    "plt.title('Model Performance')\n",
    "plt.ylabel('Accuracy score')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree visualization\n",
    "\n",
    "Requires the python library [pydot](https://pypi.python.org/pypi/pydot) and an OS package [graphviz](http://www.graphviz.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"tree.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "import pydot \n",
    "dot_data = StringIO() \n",
    "tree.export_graphviz(clf, out_file=dot_data) \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_pdf(\"tree.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image #needed to render in notebook\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation provides a method to evaluate our model performance\n",
    "\n",
    "There are a number of features that can be tuned when we build models and the data can be sliced and used in inumerable ways.  How then to measure performance of or compare different models?  Use [cross validation](http://scikit-learn.org/stable/modules/cross_validation.html) to help since it automatically does the data splits into traning and testing for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for n in range(1,10):\n",
    "    # create a model and fit it to the training data\n",
    "    clf = DecisionTreeClassifier(max_depth=n)\n",
    "    # now score it on the test data set\n",
    "    scores.append(cross_val_score(clf, X, Y, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "fig.set_size_inches(12,6)\n",
    "\n",
    "plt.boxplot(scores)\n",
    "\n",
    "plt.title('Model Performance')\n",
    "plt.ylabel('Accuracy score')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Tree Surface\n",
    "\n",
    "This example is borrowed from the [sklearn documentation](http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_classes = 3\n",
    "plot_colors = \"bry\"\n",
    "plot_step = 0.02\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
    "                                [1, 2], [1, 3], [2, 3]]):\n",
    "    # We only take the two corresponding features\n",
    "    X = iris.data[:, pair]\n",
    "    y = iris.target\n",
    "\n",
    "    # Shuffle\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.seed(13)\n",
    "    np.random.shuffle(idx)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    # Standardize\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    X = (X - mean) / std\n",
    "\n",
    "    # Train\n",
    "    clf = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.subplot(2, 3, pairidx + 1)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.xlabel(iris.feature_names[pair[0]])\n",
    "    plt.ylabel(iris.feature_names[pair[1]])\n",
    "    plt.axis(\"tight\")\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
    "                    cmap=plt.cm.Paired)\n",
    "\n",
    "#     plt.axis(\"tight\")\n",
    "\n",
    "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Switch datasets to examine the wine quality dataset hosted on the <a href=\"https://archive.ics.uci.edu/ml/datasets/Wine+Quality\">UCI website</a>. This data records 11 chemical properties (such as the concentrations of sugar, citric acid, alcohol, pH etc.) of thousands of red and white wines from northern Portugal, as well as the quality of the wines, recorded on a scale from 1 to 10. We'll only look at the data for *red* wine.\n",
    "\n",
    "First we need to load the dataset and do some prepartory work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the target from a score to a simple 'good' or 'bad' represented by a 1 or 0 and prepare the features dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = df['quality'].values\n",
    "df = df.drop('quality',1)\n",
    "Y = np.array([1 if y>=7 else 0 for y in Y])\n",
    "X = df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest adds randomness in two ways. First, it randomly resamples the data with replacement, so each decision tree is being fit on a slightly different set of data. Secondly, for each split in each decision tree, the random forests algorithm only considers a random subset of variables to split on. All trees are trained independently of each other. To make predictions, all trees are queried independently and the majority vote wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = []\n",
    "\n",
    "for ne in range(1,11):\n",
    "    clf = RandomForestClassifier(n_estimators = ne)\n",
    "    score_list = cross_val_score(clf, X, Y, cv=10)\n",
    "    scores.append(score_list)\n",
    "\n",
    "plt.boxplot(scores)\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Classification score')\n",
    "plt.title('Classification score as a function of the number of trees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What trend can you see in the results? Do you think there could be any issues with this dataset and using a Random Forest classification?  What if we just flipped a coin to try and classify wine as good or bad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blind_guess_score = sum(1-Y)/float(len(Y))\n",
    "print 100*float(sum(1-Y))/len(Y)\n",
    "\n",
    "plt.boxplot(scores)\n",
    "plt.axhline(y=blind_guess_score,ls='--')\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Classification score')\n",
    "plt.title('Classification score as a function of the number of trees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Evaluation Metrics\n",
    "\n",
    "When there are unbalanced classes in a dataset, guessing the more common class will often yield very high accuracy. For this reason, we usually want to use different metrics that are less sensitive to imbalance when evaluating the predictive performance of classifiers. These metrics were originally developed for clinical trials, so to keep with the standard terminology, we define \"good\" wines (value of 1) as \"positive\" and the \"bad\" wines (value of 0) as the \"negatives\". We then define the following:\n",
    "\n",
    "$P$ - number of true positives in the sample.\n",
    "\n",
    "$N$ - number of true negatives in the sample.\n",
    "\n",
    "$TP$ - number of true positives: how many of the \"positive\" guesses of the classifier are true.\n",
    "\n",
    "$FP$ - number of false positives: how many of the \"positive\" guesses of the classifier are actually negatives.\n",
    "\n",
    "$TN$ - number of true negatives; similarly, this is how many of the \"negative\" guesses of the classifier are true.\n",
    "\n",
    "$FN$ - number of false negatives; how many of the \"negative\" guesses are actually positives.\n",
    "\n",
    "When calling the score functions in scikit-learn you obtained the default measure of efficiency, which is called **accuracy**. This is simply the ratio of successful guesses (both positives and negatives) across all samples:\n",
    "$$\\text{accuracy} = \\frac{TP + TN}{P+N}.$$\n",
    "In our case, when the two classes (good and bad wines) are very unbalanced in the sample, we should look for a better measure of efficiency. \n",
    "\n",
    "Usually, the goal is to identify the members of the positive class (the rare class) successfully -- this could be either the good wines or the patients presenting a rare disease. It is common practice to define the following ratios:\n",
    "\n",
    "The **recall** rate (also called the sensitivity or the true positive rate) is the ratio of true positive guesses among all positives:\n",
    "$$\\text{recall} = \\frac{TP}{P}=\\frac{TP}{TP+FN}.$$\n",
    "The **precision** is the ratio of the true positive guesses over all the positive guesses:\n",
    "$$\\text{precision} = \\frac{TP}{TP+FP}.$$\n",
    "\n",
    "### Recall and Precision Examples\n",
    "\n",
    "Recall example: airport security screening, where you want a strategy that pulls aside all malicious passengers, even if it means that you also end up pulling aside many innocent people.\n",
    "\n",
    "Precision example: when selecting job applicants for on-site interviews (which take a lot of time) you want to make sure that every selected applicant is good, and you don't care as much about interviewing every good applicant.\n",
    "\n",
    "### F1 score\n",
    "\n",
    "Because precision and recall both provide valuable information about the quality of a classifier, we often want to combine them into a single general-purpose score. The **F1** score is defined as the harmonic mean of recall and precision:\n",
    "$$F_1 = \\frac{2\\times\\text{recall}\\times\\text{precision}}{\\text{recall} + \\text{precision}}.$$\n",
    "\n",
    "The harmonic mean of two numbers is closer to the smaller of the two numbers than the standard arithmetic mean. The F1 score thus tends to favor classifiers that are strong in both precision and recall, rather than classifiers that emphasize one at the cost of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# repeat of previous but with F1 scoring instead\n",
    "scores = []\n",
    "\n",
    "for ne in range(1,11):\n",
    "    clf = RandomForestClassifier(n_estimators = ne)\n",
    "    score_list = cross_val_score(clf, X, Y, cv=10, scoring='f1')\n",
    "    scores.append(score_list)\n",
    "\n",
    "plt.boxplot(scores)\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('F1 score')\n",
    "plt.title('F1 Scores as a function of the number of trees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Random Forest might not actually be good at classifying wine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Random Forest Models\n",
    "\n",
    "Random forests allow us to compute a heuristic for determining how \"important\" a feature is in predicting a target. This heuristic measures the change in prediction accuracy if we take a given feature and permute (scramble) it across the datapoints in the training set. The more the accuracy drops when the feature is permuted, the more \"important\" we can conclude the feature is. Importance can be a useful way to select a small number of features for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=15)\n",
    "\n",
    "clf.fit(X,Y)\n",
    "importance_list = clf.feature_importances_\n",
    "name_list = df.columns\n",
    "importance_list, name_list = zip(*sorted(zip(importance_list, name_list)))\n",
    "plt.barh(range(len(name_list)),importance_list,align='center')\n",
    "plt.yticks(range(len(name_list)),name_list)\n",
    "plt.xlabel('Relative Importance in the Random Forest')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Relative importance of Each Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_decision_surface(clf, X_train, Y_train):\n",
    "    plot_step=0.1\n",
    "    \n",
    "    if X_train.shape[1] != 2:\n",
    "        raise ValueError(\"X_train should have exactly 2 columnns!\")\n",
    "    \n",
    "    x_min, x_max = X_train[:, 0].min() - plot_step, X_train[:, 0].max() + plot_step\n",
    "    y_min, y_max = X_train[:, 1].min() - plot_step, X_train[:, 1].max() + plot_step\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    clf.fit(X_train,Y_train)\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "    else:\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Reds)\n",
    "    plt.scatter(X_train[:,0],X_train[:,1],c=Y,cmap=plt.cm.Paired)\n",
    "    plt.show()\n",
    "    \n",
    "# your code here\n",
    "imp_cols = clf.feature_importances_.argsort()[::-1][0:2]\n",
    "X_imp = X[:,imp_cols]\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(),\n",
    "               RandomForestClassifier(n_estimators=15)]\n",
    "\n",
    "titleClassifer = ['Decision Tree Classifier', 'Random Forest Classifier']\n",
    "for c in xrange(2):\n",
    "    plt.title(titleClassifer[c])\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plot_decision_surface(classifiers[c], X_imp, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This surface only uses the top two features selected by importance from the Random Forest classifier.  How could we include more features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves\n",
    "\n",
    "Example TBD..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
