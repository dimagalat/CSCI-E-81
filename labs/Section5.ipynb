{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# special IPython command to prepare the notebook for matplotlib\n",
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "\n",
    "# special matplotlib command for global plot configuration\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['figure.dpi'] = 150\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['font.size'] = 14\n",
    "rcParams['patch.edgecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Section 5: Clustering Continued and NLP\n",
    "\n",
    "###Last time\n",
    "- GMM for Clustering\n",
    "- EM algorithm\n",
    "\n",
    "\n",
    "###Today\n",
    "- silhouette_score for evaluating cluster models\n",
    "- NLP with nltk and sklearn\n",
    "\n",
    "**Before you begin, you should download the resources associated with the nltk library.**  Open a terminal, and install the nltk package if you haven't already with \n",
    "\n",
    "```\n",
    "conda install nltk\n",
    "```\n",
    "\n",
    "Then open a python shell (preferably your python 3.4 shell) and install the resources with \n",
    "\n",
    "```\n",
    "import nltk\n",
    "nltk.download()\n",
    "```\n",
    "\n",
    "This should launch a download application that looks like the image below.  Select the 'book' material and click download. It could take a little bit of time to complete. \n",
    "\n",
    "<img src='nltk_download.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Clustering review\n",
    "\n",
    "We'll start with an example used in the last section.  Old Faithful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "faithful = sm.datasets.get_rdataset(\"faithful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "faithful.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_faithful = faithful.data\n",
    "old_faithful.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(old_faithful.eruptions, bins = np.arange(1,6,.2), normed=True)\n",
    "plt.xlabel('Duration of Eruption (min)')\n",
    "plt.show()\n",
    "plt.hist(old_faithful.waiting, bins = np.arange(40,100,2), normed=True)\n",
    "plt.xlabel('Time between eruptions (min)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = np.array(old_faithful.eruptions), np.array(old_faithful.waiting)\n",
    "c = np.array([x,y])\n",
    "c = c.T\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('Eruption duration (mins)')\n",
    "plt.ylabel('Time between eruptions (mins)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Scikit-learn\n",
    "#Initializes with zero means and identity covariances of components\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GMM.html\n",
    "from sklearn import mixture\n",
    "\n",
    "gm = mixture.GMM(n_components=2, n_iter=100, covariance_type='full')\n",
    "print(\"initialized mixing weights of each component \")\n",
    "print(gm.weights_)\n",
    "\n",
    "gm.fit(c)\n",
    "classes = gm.predict(c)\n",
    "# print('')\n",
    "# print classes\n",
    "plt.scatter(c[:,0], c[:,1], c=classes, marker='+', s=100, linewidths=2)\n",
    "plt.title('Scikit-Learn Solution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"mixing weights after fit\")\n",
    "print(gm.weights_)\n",
    "print(\"means for each component after fit\")\n",
    "print(gm.means_)\n",
    "print(\"Covariances of components after fir\")\n",
    "print(gm.covars_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Silhouette Score\n",
    "\n",
    "Note that this link doesn't work but it will get you close!\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Silhouette_(clustering) entry for Sihouette score.  \n",
    "\n",
    "[Silhouette score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) from Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the silhouette score to evaluate a particular cluster model.  Silhouette scores can be used to help evaluate the appropriate number of clusters that are truly in the data.  The value that is returned is an indicator of how closely grouped the data is.  Values close to 1 mean that the data are appropriately clustered.  If the returned value is close to -1 then the data more likely belongs in the neighboring cluster.  A value of 0 indicates that the data resides on the border of two natural clusters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gm = mixture.GMM(n_components=2, n_iter=100, covariance_type='full')\n",
    "gm.fit(c)\n",
    "classes = gm.predict(c)\n",
    "silhouette_score(c, classes, metric='sqeuclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # build the model and fit the data\n",
    "    gm = mixture.GMM(n_components=n_clusters, n_iter=100, covariance_type='full')\n",
    "    gm.fit(c)\n",
    "    classes = gm.predict(c)\n",
    "    silhouette_avg = silhouette_score(c, classes, metric='sqeuclidean')\n",
    "    print('For n_clusters =', n_clusters, 'the average silhouette_score is', silhouette_avg)\n",
    "    \n",
    "    plt.scatter(c[:,0], c[:,1], c=classes, marker='+', s=100, linewidths=2)\n",
    "    plt.title('Solution with %d clusters' % (n_clusters))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Natural Language Processing (NLP)\n",
    "\n",
    "A quick introduction to NLP with Python using [nltk](http://www.nltk.org) and Scikit-learn.\n",
    "\n",
    "**Start with some basic string processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monty = \"Monty Python's Flying Circus. \" \n",
    "monty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monty*2 + \" Plus just the last word:\" + monty[-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monty.find('Python') #finds position of substring within string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monty.upper() +' and '+ monty.lower() # turn to upper or lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monty.replace('y', 'x') # replace letter y in the string with letter x. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regular Expressions** \n",
    "\n",
    "If you plan to work with text, [regular expressions](https://docs.python.org/2/library/re.html) are extremely useful tools to become familar with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = 'onomatopoeia'\n",
    "len(re.findall(r'[aeiou]', word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import text4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List Comprehension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the command below do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set([word.lower() for word in text4 if len(word) > 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the set method do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(['a', 'b', 'a', 'c', 'b', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[element.upper() for element in text4[0:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words in context**\n",
    "\n",
    "NLTK books are Text objects that have many built-in methods available like searching for results and also returning the context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = nltk.Text('This is some text that could be a sentence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text4.concordance(\"America\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4.similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text4.similar('citizen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4.common_contexts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text4.common_contexts(['America', 'freedom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.draw.dispersion import dispersion_plot\n",
    "dispersion_plot(text4, [\"citizens\", \"democracy\", \"freedom\", \"war\", \"America\", \"vote\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Simple Statistics **\n",
    "\n",
    "- Frequency distribution of words.  Find the counts for each word in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "freq_dist = FreqDist(text4)\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Access the entire list of words in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = list(freq_dist.keys()) # list of all the distinct types in the text\n",
    "vocabulary[:3] # look at first 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- identify specific sets of words (e.g. long words) to help characterize the body of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = set(text4)\n",
    "long_words = [word for word in words if len(word) > 15]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Lexical Resources from nltk\n",
    "\n",
    "NLTK provides several corpora (linguistic annotations, POS tags, named entities, syntactic structures, semantic roles, etc.) along with convenient methods to access these resources. The full list of corpora resources is available [here](http://www.nltk.org/book/ch02.html#tab-corpora)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words by genre\n",
    "\n",
    "The [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus) is a text collection that contains over 500 samples of english text that have been labeled with part-of-speech (POS) and genre.  It has been one of the most widely used collections in computational linguistics (statistical modeling of natual language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((genre, word) \n",
    "                               for genre in brown.categories() \n",
    "                               for word in brown.words(categories=genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of simply looking at the frequency distribution of words in a text, these methods allow inspection of word frequency distribution by genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genre_word = [(genre, word) \n",
    "              for genre in ['government', 'religion'] \n",
    "              for word in brown.words(categories=genre)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genre_word[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genre_word[-4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new condiation frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(genre_word)\n",
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cfd['religion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd['religion'].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Another corpus of stopwords is also included in the nltk resources.  These are the high-frequency words like 'the', 'to', 'of', etc. that have little lexical content and contribute little to a text's distinguising characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove the stopwords from the text and then rerun the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_genre_word = [word \n",
    "                       for word in genre_word \n",
    "                       if word[1].lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_cfd = nltk.ConditionalFreqDist(filtered_genre_word)\n",
    "filtered_cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_cfd['religion'].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation\n",
    "\n",
    "We can also remove punctuation from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_genre_word = [word \n",
    "                    for word in filtered_genre_word \n",
    "                    if word[1] not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_cfd = nltk.ConditionalFreqDist(clean_genre_word)\n",
    "clean_cfd['religion'].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming using NLTK\n",
    "\n",
    "Stemming is a reference to the process of reducing inflected or derived words to their word stem, base or root form.  There are several algorithms for stemming available today.  Most stemming algorithms function through the use of a lookup table, which is simple and effecient for languages like english but could prove difficult for other languages where inflection plays a bigger role. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stem_text = 'cats catlike catty cat stemmer stemming stemmed stem fishing fished fisher fish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stem_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The text must first be broken up into tokens.  We can use the word_tokenize method here.  There's also a method [sent_tokenize](http://www.nltk.org/api/nltk.tokenize.html) to break up larger pieces of text into sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmer\n",
    "\n",
    "The [Porter Stemmer](http://tartarus.org/~martin/PorterStemmer/) is one of the more widely used stemming algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "stemmed = [porter.stem(t) for t in tokens]\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball stemmer\n",
    "\n",
    "The [snowball stemmer](http://snowball.tartarus.org/) is based on a language that was developed specifically for stemming algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snowball = nltk.SnowballStemmer('english')\n",
    "snowball_stemmed = [snowball.stem(t) for t in tokens]\n",
    "snowball_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster stemmer\n",
    "\n",
    "The [Lancaster stemmer](http://www.lancaster.ac.uk/scc/) was developed at the University of Lancaster from where it gets its name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "lancaster_stemmed = [lancaster.stem(t) for t in tokens]\n",
    "lancaster_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity with TF-IDF\n",
    "\n",
    "The tf-idf (term frequency-inverse document frequency) is used to weigh how important a word of a document is in a document collection. It is often used as a weighting factor in information retrieval and data mining. So, tf-idf weight for a term is the product of its term frequency (tf) weight and inverse document freqency (idf) weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some simple text and calculate a comparison matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(min_df=1)\n",
    "tfidf = vect.fit_transform([\"New Year's Eve in New York\",\n",
    "                            \"New Year's Eve in London\",\n",
    "                            \"York is closer to London than to New York\",\n",
    "                            \"London is closer to Bucharest than to New York\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine=(tfidf * tfidf.T).A\n",
    "print(cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##NLP tools and beyond\n",
    "\n",
    "This is fun, but what is next?\n",
    "\n",
    "[Text clustering](https://gist.github.com/xim/1279283).  Try downloading this code and executing it on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
